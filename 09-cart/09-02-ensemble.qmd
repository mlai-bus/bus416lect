---
title: "ch09-02-ensembles"
author: "Dr. Jeff Jacob"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
library(mlbench)
library(tidyverse)
library(skimr)
library(rpart.plot)
library(yardstick)
library(mlbadata)
library(yardstick)
```

```{r}
bank.df <- mlbadata::UniversalBank
skim(bank.df)
#Let's remove ID and Zip
bank.df <- bank.df %>% 
  select(-c(1,5))
# convert loan application to factor with 1 as the target class
bank.df$Personal.Loan <- factor(as.character(bank.df$Personal.Loan),
                                  levels = c("1", "0"),
                                  labels = c("Yes", "No"))
bank.df <- bank.df %>% 
  rename(Loan = Personal.Loan )
str(bank.df)
#Do a 60-40 split
set.seed(1)
indTrain <- createDataPartition(bank.df$Loan, p = 0.6, times = 1, list = F)
train.df <- bank.df[indTrain,]
test.df <- bank.df[-indTrain,]
```

Thus far, we have looked at single trees. The advantage is that one can derive simple rules to classify new records. A big disadvantage is that single trees can be sensitive to the variables over which the first split happens. Better performance can be obtained when results are combined from multiple trees. One of the first implementations of this is bagging. Bagging refers to ***B***ootstrapped ***A*****gg**regation

## Bagged Tree

The basic idea of bagged trees is the following:

1.  Generate m bootstrapped samples of the data.

2.  Train a full tree on each sample.

3.  Each tree in the ensemble is used to classify a new record.

4.  Majority class is assigned to the record.

Bagged trees are shown to have a better predictive performance than unbagged trees. Let us make a default bagged tree using `caret` package. Note that there in no hyperparameter tuning with bagged trees in caret.

```{r}
cntrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3,
                      classProbs = T,
                      summaryFunction = twoClassSummary,
                      allowParallel = TRUE)
set.seed(1089)
tree.bag <- train(Loan ~ . ,
                       data = train.df,
                       method = "treebag",
                       trControl = cntrl,
                       #Use the ROC metric
                       metric = "ROC")
tree.bag$results

################################
# Obtain predictions

pred.class.bag <- predict(tree.bag, newdata = test.df)

confusionMatrix(pred.class.bag, test.df$Loan)
# To get AUC, predict the probabilities


ensemble_bag <- bind_cols(predClass = predict(tree.bag, newdata = test.df),
                      predProb = predict(tree.bag, newdata = test.df, type = "prob")$"Yes",
                      Actual = test.df$Loan,
                      model = "tree_bagged")


ensemble_bag |> roc_auc(Actual, predProb)
```

We get an AUC of `{r} tree.caret.bag.cv$results$ROC`, (0.99), here )which is very high. Bagged trees improve the predictive performance by reducing the variance of the prediction.

The bagged trees however can also suffer from the a similar drawback as the single trees. While, they are more stable, the same set of predictors can exert substantial influence in growing of the trees. This is because the same predictors are considered at each split in designing the tree. While the underlying data may be different for each run of the tree, the trees are not independent of each other and may have structures similar to each other.

This tree correlation can be reduced by adding randomness to the construction of the trees.

## Random Forests

In random forests, multiple samples are drawn like in bagging but at each split in constructing every tree, a subset of m predictors are randomly chosen. Typically, this number is one-third of the total number of predictors.

In `caret`, this number can become a tuning parameter with the number of variables being five evenly spaced values between 2 to p. Another tuning parameter is the number of trees that can be built for each sample. The default in the `randomForest` package, which is used by `caret` is 500. We can leave it at that.

**A random forest built over several tuning parameters and repeated 10 fold cross-validation will take a few hours to run.** One quick way to reduce time is to limit the number of cross validation folds and repeats. One could also limit the number of tuning parameter values.

If these fixes are not desirable, the training model should be run taking advantage of parallel processing.

```{r echo = F, eval=FALSE}
cntrl <- trainControl(method = "repeatedcv", number = 5, repeats = 1,
                       classProbs = T,
                      summaryFunction = twoClassSummary,
                      allowParallel = TRUE)

rfGrid <- expand.grid(mtry = seq(2,10,2))
set.seed(1089)
tree.rf <- train(Loan ~ . ,
                       data = train.df,
                       method = "rf",
                       trControl = cntrl,
                       tuneGrid = rfGrid,
                       #Use the ROC metric
                       metric = "ROC")

tree.rf
tree.rf$finalModel


# We can see the results of the tuning process

tree.rf$results 

######################################
# To get AUC, predict the probabilities

ensemble_rf <- bind_cols(predClass = predict(tree.rf, newdata = test.df),
                      predProb = predict(tree.rf, newdata = test.df, type = "prob")$"Yes",
                      Actual = test.df$Loan,
                      model = "tree_rf")

ensemble_rf |> 
    roc_auc(Actual, predProb)
```

Though ensemble methods do not give us clear rules like the individual trees, we can obtain a variable importance plot that tells us the relative importance of the various predictors.

```{r}
varImp(tree.rf)
plot(varImp(tree.rf))
```

## Boosted Trees

A third approach to ensembles in constructing classification tree is boosting.

The basic idea behind the AdaBoost, which is the most common implementation of boosting, is to generate a sequence of weak classifiers. At each stage though, the records that are incorrectly classified are given more weight while those correctly classified are given less weigh. Thus, the algorithm learns from miss-classified records at each stage, focusing on difficult to classify records. The overall

The steps in the algorithm are

1.  First, the two classes are represented as -1 and +1):
2.  Give an initial weight $w^{1}_{i}$ be 1/n to each observation.
3.  Repeat the following steps for k= 1, 2, 3,...K times
    1.  Fit a weak classifier $G_m(x)$ (eg. one split decision tree), using $w^{k}_{i}$

    2.  Compute error as $$err_m = \frac{\sum_{i=1}^{N} w_i I(y_i \ne G_m(x_i))}{ \sum_{i=1}^{N} w_i} $$

    3.  Compute each tree's weight (say) as $$\alpha_m = log((1-err_m)/err_m) $$

    4.  Update weight as $$ w_{i}^{m+1} = w_i \cdot exp[\alpha _m \cdot I(y_i \ne G_m(x_i))], i = 1, 2, \ldots N  $$
4.  Output \$ G(x) = sign \left[\sum_{m=1}^{M}\alpha_m G_m(x)\right] \$

As seen above, first a weak classifier is run with each observation given equal weight. The expression $$I(y_i \ne G_m(x_i))$$ returns 1 for an incorrect classification and 0 for a correct one. Set 3.2 calculates the weighted error. In Step 3.3, the classifier's weight or say is calculated in a manner that more acurate classifiers are given a higher weight. Finally, in 3.4 individual observations weights are updated by $$ exp(\alpha_m) $$ if the prediction was an error and 1 if correct. Thus, incorrect predictions are given a greater weight.

For the final output, a weighted linear combination of all M classifiers is used. If the resulting expression is \<0, -1 class is assigned to the observation and 1 otherwise.

Some things to note are that since `Adaboost` learns from the previous trees, it needs to be sequential and can not be implemented in parallel processing.

In `caret` there are several implementations of `Adaboost`. Here, we will use the method `ada`. Tunig parameters are:

-   `iter` (#Trees)
-   `maxdepth` (Max Tree Depth)
-   `nu` (Learning Rate)

```{r}
cntrl <- trainControl(method = "cv", number = 5,
                      classProbs = TRUE,
                      summaryFunction =  twoClassSummary,
                      allowParallel = FALSE)
# The tuning parameters for boosting are: mfinal (# of trees)
# and maxdepth (the maximum depth of the tree)
# we first build the default tree
ptm <- proc.time()
set.seed(9876)
tree.adaboost1 <- train(Loan ~ . , data = train.df,
                       method = "ada",
                       trControl = cntrl,
                       tuneLength = 5,
                       metric = "ROC")
               
#tree.adaboost1 <- boosting(Loan ~ ., data = train.df)
#tree.adaboost1
proc.time() - ptm
tree.adaboost1

ensemble_ada1 <- bind_cols(predClass = predict(tree.adaboost1, newdata = test.df),
                      predProb = predict(tree.rf, newdata = test.df, type = "prob")$"Yes",
                      Actual = test.df$Loan,
                      model = "tree_ada1")

ensemble_ada1 |> 
    roc_auc(Actual, predProb)
```

The final model chosen by caret had a depth of 5 and 150 trees were built. The learning was kept constant at 0.1. One could have specified tuning parameter using the `tuneGrid =` call with more values of the two parameters.

**Gradient Boosted Machines**

Boosted tree are very powerful in terms of their classification accuracy. There are a couple of drawbacks however, in terms of computational and operational procedures. The entire dataset gets reweighted at each iteration of the learner run, based on the previous model's errors.

Gradient boosted machine improve upon boosting by fitting regression trees on the current fit's errors directly.

The model in `caret` is implemented using the `gbm` package with the following tuning parameters:

-   `n.trees` (# Boosting Iterations)

-   `interaction.depth` (Max Tree Depth)

-   `shrinkage` (Shrinkage)

-   `n.minobsinnode` (Min. Terminal Node Size)

```{r}
cntrl <- trainControl(method = "cv", number = 5,
                      classProbs = TRUE,
                      summaryFunction =  twoClassSummary,
                      allowParallel = TRUE)


ptm <- proc.time()
set.seed(9876)
tree.gbm <- train(Loan ~ . , data = train.df,
                       method = "gbm",
                       trControl = cntrl,
                       tuneLength = 5,
                       metric = "ROC")
               
 
proc.time() - ptm
tree.gbm

ensemble_gbm <- bind_cols(predClass = predict(tree.gbm, newdata = test.df),
                      predProb = predict(tree.rf, newdata = test.df, type = "prob")$"Yes",
                      Actual = test.df$Loan,
                      model = "tree_gbm")

ensemble_gbm |> 
    roc_auc(Actual, predProb)
```

Regardless, the boosted trees are a very powerful classification tool. What is interesting is that weaker the initial trees, the stronger is the ensemble.

Let's combine all the results and obtain the usual lift curve and AUCs

```{r}
ensemble_results <- bind_rows(ensemble_bag, ensemble_rf, ensemble_ada1, ensemble_gbm)

ensemble_results |> 
    group_by(model) |> 
    roc_auc(Actual, predProb) |> 
    arrange(-.estimate)
```

```{r}
ensemble_results |> 
    group_by(model) |> 
    roc_curve(Actual, predProb) |> 
    autoplot()
```

```{r}
ensemble_results |> 
    group_by(model) |> 
    lift_curve(Actual, predProb) |> 
    autoplot()
```
