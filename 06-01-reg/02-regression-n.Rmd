---
title: "02-regression"
author: "Dr. Jeff Jacob"
date: "2/6/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Variable Selection


Load the libraries

```{r}
library(tidyverse)
library(broom)
library(forecast)
library(caret)
```

Like before, we will use the first 1000 observations and some selected variables to predict the price of used Corollas

```{r}
car.df <- read_csv("ToyotaCorolla.csv")
# use first 1000 rows of data
car.df <- car.df[1:1000, ]
# select variables for regression


#convert fuel type to factor
car.df$Fuel_Type <- as.factor(car.df$Fuel_Type)

selected.var <- c(3, 4, 7, 8, 9, 10, 12, 13, 14, 17, 18)

car.df.sel <- car.df[, selected.var]
glimpse(car.df.sel)


```

Here, we will explicitly create dummy variables using one-hot encoding (leaving out one category) and then partition the data to a 60-40 split. 

```{r}

carDum <- dummyVars(Price ~ ., data = car.df.sel, fullRank = T)

car.df.dum <- as_tibble(predict(carDum, newdata = car.df.sel)) 

#dummyVars creates a matrix by default. Next, we bring in Price
car.df.dum <- cbind(car.df.sel$Price, car.df.dum) 

#Rename the first column
names(car.df.dum)[1] <- "Price"

set.seed(1)
trainInd <- createDataPartition(car.df.dum$Price, p = 0.6, list = F)
train.df <- car.df.dum[trainInd, ]
valid.df <- car.df.dum[-trainInd, ]
```



### Forward / backward and stepwise

**Forward Selection** starts with just the intercept and variables with the lowest p value are added sequentially. The algorithm stops when the contribution of additional predictors is not statistically significant. The main disadvantage of this method is that the algorithm will miss pairs or groups of predictors that perform very well together but perform poorly as single predictors.

**backward elimination** Algorithm starts with a full model and eliminates sequentially the least significant variables. The algorithm stops when only significant variables remain. Again, a disadvantage is that the model would miss interactions or relationships between predictors.

**Stepwise** or **sequential elimination** has features of both the above algorithms. It starts with forward selection. But at each step, like in backward elimination, non-significant variables are eliminated.

These stepwise procedures may do a good enough job for prediction purposes, but if the aim of the analysis explanatory modeling, the choice of variables should be driven by business/economics domain knowledge.

We will implement these algorithms in the `caret` package. The default settings in caret are to draw 25 bootstraps samples of the data with replacement, tune the model over three values of the training parameters- here,the number of predictors to be included- and then choose the model with the *lowest RMSE*. That model is put in the `finalModel` parameter. First, we will build the model with these default settings and then supply a tuning grid of 2 to 10 variables.


```{r}
set.seed(123)
forwd.1 <- train(Price  ~ ., data = train.df,
                 method = "leapForward")
forwd.1
```


The default tuning space was 2, 3 or 4 predictors excluding the intercept, and `nvmax = 4` was chosen based on  *the average RMSE from the 25 bootstrapped samples that were drawn*. In each of these cases, variables were added based on the **forward selection ** algorithm. So, there is a lot going on here.

The neat thing about caret is that this final model is saved under the `finalModel` label. This can be easily extracted and the specific variables parsed out for further model building.


```{r}

# to see what variables got selected
summary(forwd.1)

#We can view the RMSE

forwd.1$results$RMSE 
# We see that the third model has the lower RMSE

which.min(forwd.1$results$RMSE)

# to get the coefficient estimates of this model
coef(forwd.1$finalModel,3)

#or
coef(forwd.1$finalModel,which.min(forwd.1$results$RMSE))


```



Let us run forward selection on upto 10 variables.

```{r}
optimal.n <- expand.grid(nvmax = 1:10) #creates a dataset of the values to nvmax to tune
set.seed(123)
forwd.2 <- train(Price ~ . , data = train.df,
                 method = "leapForward",
                 tuneGrid = optimal.n )
forwd.2
```

Here, a model with `r which.min(forwd.2$results$RMSE)`  variables gets chosen. 

```{r}
summary(forwd.2)


```

Let us extract these names.

```{r}

which.min(forwd.2$results$RMSE)

coef(forwd.2$finalModel,which.min(forwd.2$results$RMSE))

#Let's get the names (excluding the intercept)
frd.names <- names(coef(forwd.2$finalModel, which.min(forwd.2$results$RMSE)))[-1]

# We could then only select the sample with these variables. For example
train.df.sel <- train.df[, names(train.df) %in% c(frd.names) ]

# Create a list of the variables chosen and the RMSE. This can be comapared
## across the three models.

forward <- list(vars = frd.names , rmse = min(forwd.2$results$RMSE))
```


In `caret`, it is very easy to use a different algorithm, by just changing the `method =` argument in the `train()` function. Forward, Backward and Sequential have the same tuning parameter: `nvmax`. Thus, we do not have to change the tuneGrid function.

###Backward elimination

```{r}
set.seed(123)
bckwrd <- train(Price ~ . , data = train.df,
                 method = "leapBackward",
                 tuneGrid = optimal.n)
bckwrd
which.min(bckwrd$results$RMSE)

```

Interestingly, here a model with `r which.min(bckwrd$results$RMSE)` variables are chosen. We can again obtain the name of the variables and the coefficients as well.

```{r}

which.min(bckwrd$results$RMSE)

coef(bckwrd$finalModel,which.min(bckwrd$results$RMSE))

#Let's get the names (excluding the intercept)
bckwrd.names <- names(coef(bckwrd$finalModel, which.min(bckwrd$results$RMSE)))[-1]

bckwrd.names

backward <- list(vars = bckwrd.names , rmse = min(bckwrd$results$RMSE))

```

Both backward and forward gave the same model.

### Sequential selection



```{r}
set.seed(123)
seqsel <- train(Price ~ . , data = train.df,
                 method = "leapSeq",
                 tuneGrid = optimal.n)
seqsel
which.min(seqsel$results$RMSE)
```

We can again obtain the name of the variables and the coefficients as well.

```{r}

which.min(seqsel$results$RMSE)

coef(seqsel$finalModel,which.min(seqsel$results$RMSE))

#Let's get the names (excluding the intercept)
seqsel.names <- names(coef(seqsel$finalModel, which.min(seqsel$results$RMSE)))[-1]

seqsel.names

stepwise <- list(vars =seqsel.names , rmse = min(seqsel$results$RMSE))

```

Let's combine all the results

```{r}
selection.list <- list(forw.sel = forward, back.sel = backward, seq.sel =stepwise)
selection.list
```

Though sequential or stepwise selection alrorithm does the best, it includes 10 variables. Backward/forward selections have only four variables. They wins on parsimony and do not result in a huge increase in the RMSE. So, one might lean towards this model. Let's examine each of these model's performance on the validation set, that we have not yet touched. Again, the advantage of `caret` is that the best model is already stored in that respective algorithm's object and we only need to predict the price.

```{r}

Price.for <- predict(forwd.2, newdata = valid.df)
accuracy(Price.for, valid.df$Price)

Price.back <- predict(bckwrd, newdata = valid.df)
accuracy(Price.back, valid.df$Price)

Price.seq <- predict(seqsel, newdata = valid.df)
accuracy(Price.seq, valid.df$Price)

```

