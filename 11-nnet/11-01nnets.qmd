---
title: "NeuralNets"
format: html
editor: visual
---

## Neural Networks

Neural networks (NN), also called artificial neural networks, are models for classification and prediction. The neural network is based on a model of biological activity in the brain, where neurons are interconnected and learn from experience. Neural networks mimic the way that human experts learn. The learning and memory properties of neural networks resemble the properties of human learning and memory, and they also have a capacity to generalize from particulars.

NNs have a very high predictive accuracy and their use has made possible deep learning models.

The basic idea of NN is that information from the inputs gets fed into one or more hidden layers, each with multiple nodes. The information from these nodes then gets transmitted to the output layers which has the number of nodes corresponding to the number of classes in the outcome variable.

```{r}
#| echo: false
#| output: false

library(caret)
library(tidyverse)
library(mlbadata)
library(neuralnet)
library(nnet)

# A handy neuralnet plotting function:
install.packages("NeuralNetTools")
library(NeuralNetTools)


```

### Building a Multi-Layer Perceptron (neural net) in R using scikit-learn

We will also use `python` package `scikit-learn` to create the neural nets and use functions from `caret` to evaluate model fit. Neural Nets are called *Multi-Layer Perceptron (MLP)* and we will use the `MLPClassifier` to estimate these models.

To set up the python environment:

```{r}
library(reticulate)
use_virtualenv("mlpyenv")
np <- import("numpy", convert = FALSE)
pd <- import("pandas")
sknn <- import("sklearn.neural_network")
import("sklearn.model_selection")
import("sklearn.preprocessing")
```

Let's start with a tiny dataset to motivate the creation of nns

```{r}

```

```{r}
tiny_df <- mlbadata::TinyData
glimpse(tiny_df)
str(tiny_df)

```

One requirement of nn is that the data be either \[0,1\] or \[-1,1\]. This is met here. Otherwise, the data should be rescaled as follows:

For \[0,1\]- $$ \frac{x_i-min(x)}{max(x)-min(x)}$$

For \[-1,1\] $$ 2 \times \frac{x_i-min(x)}{max(x)-min(x)} - 1$$

Both these transformations can easily be done by the `MinMaxScaler()` function of `sklearn.preprocess` API. The arguments (min (=0) , max (=1)) range the method is:

```         
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std * (max - min) + min
```

We will build a model with one input layer and three nodes. Let's start without any training and testing split.

```{r}
xvars <- c("Fat", "Salt")
X <- tiny_df[xvars]
y <- tiny_df["Acceptance"]


#Specify the model

mlpcl = sknn$MLPClassifier(hidden_layer_sizes = (3L),
                      activation = "logistic",
                      solver = "lbfgs",
                      random_state = 123L)

# Run the mode on the dataset
mlpcl$fit(X,y)

# Now, we can inspect various objects generated by mlplc$[Tab]

mlpcl$activation

#Obtain the bias and weights


print("Bias")
print(mlpcl$intercepts_)

print("Weights")
print(mlpcl$coefs_)


```

Some things to note above. The argument `hidden_layer_sizes()` expects an array of integers specifying the number of nodes in the ith layer. If you type (3), python will read it as a float (3.0) and will not run. So explicitly specify it as an integer by calling it `3L`.

Also, because we have run the above nnet in the `R` environment, we can obtain predictions and apply post estimation metrics from `caret`.

```{r}
yhat <- factor(mlpcl$predict(X))
confusionMatrix(reference = tiny_df$Acceptance, 
                data = yhat,
                positive = "like")

y_prob <- mlpcl$predict_proba(X)
y_prob
```

### Building a neural network natively in Python

We can run the same MLP model in `Python`

```{python}
tinypy = r.tiny_df
xvar = ["Fat", "Salt"]
X = tinypy[xvar]
y = tinypy["Acceptance"]
y.value_counts()

from sklearn.neural_network import MLPClassifier

mlpcl = MLPClassifier(hidden_layer_sizes = (3),
                      activation = "logistic",
                      solver = "lbfgs",
                      random_state = 123)
mlpcl.fit(X,y)

mlpcl.predict(X)

mlpcl.intercepts_
mlpcl.coefs_
```

The above is the same nnet as the one obtained in the R environment. Let's obtain the confusion matrix and accuracy scores.

```{python}
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

confusion_matrix(y, mlpcl.predict(X))

accuracy_score(y, mlpcl.predict(X))
```

There is more detailed information in `caret`.

### Building a neural network in `caret`

We can also, very easily build a single-layer neural network in caret. Note that while `caret` contains wrapper function for several neural networks, the most commonly used algorithm for classification is the `nnet` package, which only allows one hidden layer. For more elaborate networks, we will default to `MLPClassifier` in `sklearn.`

In `caret`, the convention is to label the first class as the class of interest. So we will relevel the wights

```{r}

# Let's relevel the data

tiny_df$Acceptance <- fct_relevel(tiny_df$Acceptance, "like")

set.seed(1)

fitControl <- trainControl(classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           method = "none")


nn.tune <- expand.grid(size = 3, decay = 0)
nn2 <- train(Acceptance ~ Fat + Salt, data = tiny_df,
             method = "nnet",
             trControl = fitControl,
             tuneGrid = nn.tune)

summary(nn2$finalModel)
yhat2 <- predict(nn2, newdata = tiny_df)
confusionMatrix(reference = tiny_df$Acceptance,
                data = yhat2)


```

Finally, we can plot the nnet:

```{r}
summary(nn2)
plotnet(nn2$finalModel)
```

In the plot, positive values come in as black lines, negative as gray and the thickness depends on magnitude. Thus, the weight from salt (input 2) to the node 1 of hidden layer , $$ w^{1}_{1,2} $$ , is 38.36 (given by the value corresponding to `i2 -> h1`).
