---
title: "NeuralNets"
format: html
editor: visual
---

## Neural Networks- 2

Here we will build a neural network on a larger dataset.

```{r}
#| echo: false
#| output: false

library(caret)
library(tidyverse)
library(mlbadata)
library(broom)
library(skimr)
library(fastDummies)
```

To set up the python environment:

```{r}
library(reticulate)
use_virtualenv("mlpyenv")
np <- import("numpy")
pd <- import("pandas")
sknn <- import("sklearn.neural_network")
import("sklearn.preprocessing")
```

We will work with the accident dataset

```{r}
accdnt_df <- mlbadata::AccidentsNN
skim(accdnt_df)
```

Let's inspect the values in the dataframe

```{r}
accdnt_df %>% map(unique)
```

We need to do some preprocessing steps. Alcohol (yes =1, no = 2), sur_cond (dry =1, wet = 2, snow/slush = 3, ice = 4, unknown = 9) needs to be converted to dummies with 9 dropped, max_sev_ir (no injury = 0, injury = 1, fatality = 2) needs to be converted to a factor variable. Vehicle involved needs to be rescaled

```{r}
clean_df <- accdnt_df %>% filter(SUR_COND != 9) %>% 
  mutate(ALCHL_I = factor(ALCHL_I, 
         levels = c("2","1"),labels = c("no", "yes")),
         MAX_SEV_IR = factor(MAX_SEV_IR, levels = c("0", "1", "2"),labels = c("no injury", "injury", "fatality")),
         VEH_INVL = ((VEH_INVL - min(VEH_INVL))/(max(VEH_INVL)- min(VEH_INVL))))  %>% 
  dummy_columns(select_columns = c("SUR_COND", "ALCHL_I") ,      remove_selected_columns = TRUE, remove_first_dummy = FALSE) 
```

Notice that we did not remove the first dummy of each category. This method is called one-hot encoding. Even though it causes multicollinearity, neural nets do not involve inverting the feature matrix and are thus not affected by multicollinearity.

We will also create a training and testing split.

```{r}
glimpse(clean_df)

set.seed(123)
idx <- createDataPartition(clean_df$MAX_SEV_IR, p = 0.6, list = FALSE)
train_df <- clean_df[idx,]
test_df <- clean_df[-idx,]
```

Now we split the data into X matrix and y and pass it into MLPClassifier

```{r}
train_X <- train_df %>% select(-MAX_SEV_IR)
train_y <- train_df %>% select(MAX_SEV_IR)

test_X <- test_df %>% select(-MAX_SEV_IR)
test_y <- test_df %>% select(MAX_SEV_IR)




#Specify the model
# Start with 8 nodes, one for each input

mlpcl = sknn$MLPClassifier(hidden_layer_sizes = (8L),
                      activation = "logistic",
                      solver = "lbfgs",
                      random_state = 123L)

# Run the mode on the dataset
mlpcl$fit(train_X,train_y$MAX_SEV_IR)

# Now, we can inspect various objects generated by mlplc$[Tab]

mlpcl$activation

#Obtain the bias and weights


print("Bias")
print(mlpcl$intercepts_)

print("Weights")
print(mlpcl$coefs_)


```

Some things to note above. The argument `hidden_layer_sizes()` expects an array of integers specifying the number of nodes in the ith layer. If you type (3), python will read it as a float (3.0) and will not run. So explicitly specify it as an integer by calling it `3L`.

Also, because we have run the above nnet in the `R` environment, we can obtain predictions and apply post estimation metrics from `caret`.

```{r}
yhat <- factor(mlpcl$predict(train_X), 
               levels = c("no injury", "injury", "fatality"))
confusionMatrix(reference = train_y$MAX_SEV_IR, 
                data = yhat)



```

Let's apply this to the test set

```{r}


yhat_test <- factor(mlpcl$predict(test_X),
                    levels = c("no injury", "injury", "fatality"))
confusionMatrix(reference = test_y$MAX_SEV_IR, data = yhat_test)
```

The training set had an accuracy of 0.88 while the test set has an accuracy of 0.86.

We can experiment with a different number of nodes, adding an additional hidden layer, including a learning rate etc to improve on the predictive power of the neural net.

### Building a neural network in Python

Like before, we can build the exact same model in `Python` . While there are built in functions to create train test split, we can just bring in the partition that we created in R

```{python}

train_X = r.train_X
train_y = r.train_y

test_y = r.test_y
test_X = r.test_X

from sklearn.neural_network import MLPClassifier

mlpcl = MLPClassifier(hidden_layer_sizes = (8),
                      activation = "logistic",
                      solver = "lbfgs",
                      random_state = 123)
mlpcl.fit(train_X,train_y)


mlpcl.intercepts_
mlpcl.coefs_
```

The above is the same nnet as the one obtained in the R environment. Let's obtain the confusion matrix and accuracy scores.

```{python}
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

confusion_matrix(train_y, mlpcl.predict(train_X))

accuracy_score(train_y, mlpcl.predict(train_X))
```

## \[Adv.\] Hyperparameter tuning in MLP

There are several parameter that can be tuned by the user in an MLP model:

1.  The number of hidden layers
2.  The number of nodes in each hidden layer
3.  The activation function- {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’
4.  Optimization algorithm- {lbfgs’, ‘sgd’, ‘adam’}, default=’adam’
5.  The learning rate - {‘constant’, ‘invscaling’, ‘adaptive’}, default=’constant’

Let us tune the model with some of these hyperparameters. Like in `caret,` we will set up a grid search space and then optimize using a 5-fold cross validation. The main arguments for the grid search are the estimator, parameter space, number of CPU cores to use and the number of folds of the cross-validation. Note: We will do this in `Python`.

```{python}


# define the model and cap iterations (epochs) to 10 to save time

train_X = r.train_X
train_y = r.train_y

test_y = r.test_y
test_X = r.test_X

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV

mlpcl = MLPClassifier(max_iter = 100)

# define the grid space

mlp_grid = {
    'hidden_layer_sizes': [(8,),(5,), (3,), (5,3) ],
    'activation': ['logistic', 'relu'],
    'solver': ['lbfgs', 'adam'],
    'learning_rate': ['constant' , 'adaptive'] 
}

mlpcv = GridSearchCV(mlpcl, mlp_grid, cv =5)

mlpcv.fit(train_X, train_y)
```

To see the results of the final model:

```{python}
print('Best parameters found from grid search:\n', mlpcv.best_params_)
```

We can now use this model to predict the training and test set

```{python}
confusion_matrix(train_y, mlpcv.predict(train_X))

accuracy_score(train_y, mlpcv.predict(train_X))

test_yhat = mlpcv.predict(test_X)
print("Accuracy of the test set with the best model: \n" , accuracy_score(test_y,test_yhat ))

confusion_matrix(test_y, test_yhat, labels = mlpcv.classes_)
```

We see that the best model, given by the following parameter:

```         
Best parameters found from grid search:
 {'activation': 'logistic', 'hidden_layer_sizes': (5,3), 'learning_rate': 'adaptive', 'solver': 'lbfgs'}
```

is as good as the one that we ran initially (1 hidden layer with 8 nodes, constant learning rate). We can get the full featured confusion matrix from `caret`:

```{r}

test_yhat <- py$test_yhat

test_yhat <-  test_yhat%>% 
                        data.frame("MAX_SEV_IR" = factor(test_yhat, 
                         levels = c("no injury", "injury", "fatality")))

  
confusionMatrix(reference = test_y$MAX_SEV_IR, data = test_yhat$MAX_SEV_IR)

```
