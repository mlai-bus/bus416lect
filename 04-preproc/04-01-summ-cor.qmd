---
title: "04-01-summ-cor"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Cleaning

A first step to any analysis is examining the descriptive statistics from the dataframe and so some preliminary pre-processing.

Load the required libraries

```{r}
#| echo: false


library(tidyverse)
library(caret)
library(corrplot)
library(psych)
library(hablar)
library(skimr)
library(purrr)
library(readr)
library(patchwork)
library(mlbadata)
```

Load the dataset and convert two variable to factors. We will learn how to use the `apply()` family of functions which apply functions in r to various elements of a list.

```{r}

housing.df <- mlbadata::BostonHousing
# convert CHAS and CAT..MEDV to factors

cols <- c("CHAS", "CAT.MEDV")

housing.df[cols] <- map(housing.df[cols], 
                        factor, levels = c(1, 0),
                        labels = c("Yes", "No"))


head(housing.df)
```

*Note about variable names*: Generally, variable names should not contain spaces. If `read.csv()` is used, a `.` will be inserted instead of spaces. On the other hand, when `read_csv` is used, the spaces between variable names is retained. So when referring to these variables in `dplyr` functions, which expect the name to be *unquoted*, you need to enclose the variable names in back-ticks, for example, `` `CAT MEDV` ``.

There are two-factor variables in the above dataset: CHAS and CAT MEDV. We first create a vector with these two columns and then apply (map) the factor function to the list of these two variables. The syntax of `map()` function is:

```         
map(.x, .f, â€¦)
```

The `...` means that various arguments from the chosen `f.` or the function can be passed here. Thus, we first made 1 as the first level and then added the labels Yes and No for 1 and 0 respectively. Note, this format is important. We will do machine learning using the `caret()` package. The default setup for caret is that class 1 will be the reference class.

```{r}
glimpse(housing.df)
```

Looking at descriptive stats:

```{r}
summary(housing.df)
# a better format
skim(housing.df)

# customize the output
my_skim <- skim_with(numeric = sfl(p25 = NULL, p75 = NULL)) 
#sfl: skim function list
my_skim(housing.df)

```

We can also create pivot tables. Suppose we want to see average median price by RAD and Charles River.

```{r}

housing.df %>% summarise(avgVal = mean(MEDV, na.rm = T))

housing.df |> 
  group_by( RAD, CHAS) |> 
  summarise( averageValue = mean(MEDV, na.rm = T),
             count = n())


```

**Note:** As seen above, we can create Excel-type pivot tables very easily using the `dplyr` functions.

Another critical aspect of pre-processing is creating factor variables and interactions. Here, we have two dummy variables, CHAS and CAT MEDV. Let us create a dummy variable for CHAS and the interaction of CHAS and ROOMS

```{r}
df1 <- dummyVars(~ CHAS + CHAS:RM, data = housing.df)
df2 <- dummyVars(~ CHAS , data = housing.df)
dummies.df1 <- predict(df1, newdata = housing.df)

dummies.df2 <- predict(df2, newdata = housing.df)

head(dummies.df2)
```

Notice that in the above specification, two dummies have been created for `CHAS`, that is, one for each category. If we included both of them in linear regression, we have the *dummy variable trap* or *perfect multicollinearity.* We need to exclude one of the categories. In other words, we need to create a matrix with *full* rank. It is easy in `caret`:

```{r}
df2 <- dummyVars(~ CHAS + CHAS:RM, data = housing.df,
                 fullRank = T)

dummies.df2 <- predict(df2, newdata = housing.df)
head(dummies.df2)
```

In both these cases, `caret` created a separate data from with the dummy and interaction variables. We can now create a new dataframe where we combine the new dataset with the housing.df

```{r}

housing.df1 <- cbind(housing.df, dummies.df2) |> 
  select(-CHAS)
head(housing.df1)
```

## Dimension reduction: Correlation analysis

Having a high correlation between predictor variables can result in multicollinearity in regression-based models. Before any modelling is undertaken. If two variables are highly correlated (say, above a certain threshold like 0.8), the one which is highly correlated with other variables should be removed.

`caret` has a built-in function, `findCorrelation()` which follows this algorithm in removing such highly correlated variables.

We will first apply some data-cleaning methods. We will separate out our dependent variables (MEDV and CAT MEDV) into a separate dataset, remove categorical variables from the `xvariable` dataset and then compute correlations.

```{r}
yvars <- housing.df |> 
  select(MEDV, CAT.MEDV)

xvars <- housing.df |> 
  select(-c(MEDV, CAT.MEDV))

xvarsnum <- xvars |> 
  select_if(is.numeric) #CHAS will get ommited
# if there were multiple categorical variables, we could have pulled them as !na.numeric or is.factor

cor1 <- cor(xvarsnum)
cor1

corrplot(cor1)
corrplot(cor1, method = "number")
# organized by clusters of high correlation
corrplot(cor1, order = "hclust" , method = "number")

# This method is cool but gets hard to read with several variables.
library(GGally)
ggpairs(xvarsnum[, c(1, 3, 4, 6)])
```

We do see above that there are some variable with rather high pair wise correlation. Let's identify and remove them.

```{r}
high.corr <- findCorrelation(cor1, cutoff = 0.8)
high.corr

t(names(xvarsnum[high.corr]))

#var is TAX. Next we remove it
xvarfin <- xvarsnum[, -high.corr]
```

Combine this dataset with CHAS and MEDV

```{r}
df1 <- cbind(MEDV = yvars$MEDV, CHAS = xvars$CHAS, xvarfin)
head(df1)
```

## 
