---
title: "10-01-logistic"
format: html
editor: visual
error: true
echo: true
---

Note: This is the updated project file.

## Ch 10: Logistic Regression

Classification refers to the idea of assigning a success or failure class (in the case of binary outcomes) to to an observation. This class assignment is based on generating a probability of the outcome for that case and then classifying the record using a probability cutoff value. Since, we need to generate probabilities, we can not use the traditional ordinary least squared regression approach there is nothing in the model that restricts the predictions to be between 0 and 1. Logistic regression is an approach that helps us get around that.

This file introduces logistical regression using the default `glm()` syntax.

```{r}
#| echo: true
#| message: false
#| warning: false
library(tidyverse)
#library(gains)
library(caret)
library(yardstick)
library(broom)
library(mlbadata)
```

Let us first model the acceptance of personal loan as a function on income. Let `p` denote the probability of accepting a personal loan. The linear model is given by

$p = \beta_0 + \beta_1 income$

The *logistic response function* is given by:

$p = \frac{1}{1 + \exp^{-(\beta_0 + \beta_1 income)}}$

We will estimate the response from both the models and plot the resulting curve.

For estimation, we will use the glm() function from base r. `glm()` treats the *second level* as the target level.

```{r}

toy.df <- mlbadata::UniversalBank[,c(4,10)]
toy.df$Class <- factor(toy.df$Personal.Loan, 
                                levels = c(0,1),
                                labels = c("No", "Yes"))



reg.1 <- lm(Personal.Loan ~ Income, data = toy.df)
tidy(reg.1)


#Now logit
logit.1 <- glm(Class ~ Income, family = "binomial", data = toy.df)
tidy(logit.1)
```

```{r}
# We first define the two functions

lin.mod <- function(x){
  reg.1$coefficients[1] + reg.1$coefficients[2]*x
}

logit.mod <- function(x){
  (1 / (1 + exp(-(logit.1$coefficients[1] +  logit.1$coefficients[2]*x))))
}

toy.df %>% 
  ggplot(aes(x = Income, y = Personal.Loan)) +
  geom_jitter(width=0.01, height=0.02, alpha=0.1) +
  geom_function(fun = lin.mod, color = "blue") +
  geom_function(fun = logit.mod, color = "red") +
  labs(x = "Income in '000s",
       y = "Personal Loan")
  
```

### Logistic Regression: full model

In general, the *logistic response function* is given by:

$$p = \frac{1}{1 + \exp^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 \dots + \beta_K x_K)}}$$

If we define odds of success as: $$Odds( Y = 1) = \frac{p}{1-p}$$

Then, with some transformation, p equals: $$p = \frac{odds}{1+ odds}$$

Substituting the above expression into our logistic response function, we can obtain:

$$ Odds( Y = 1) = \exp^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 \dots + \beta_K x_K} $$ Or, taking log on both sides, $$ \log(odds) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \dots + \beta_K x_K$$

This is the equation of the standard form of the logistic model that is estimated by `glm()` under the option of `family = "binomial"`.

Let us now estimate our model on the full dataset.

```{r}

bank.df <- mlbadata::UniversalBank
bank.df <- bank.df[ , -c(1, 5)]  # Drop ID and zip code columns.
# treat Education as categorical (R will create dummy variables)
bank.df$Education <- factor(bank.df$Education, levels = c(1, 2, 3), 
                            labels = c("Undergrad", "Grad", "Adv/Prof"))

bank.df$Personal.Loan <- factor(bank.df$Personal.Loan, 
                                levels = c(0,1),
                                labels = c("No", "Yes"))

glimpse(bank.df)
# partition data
set.seed(1111)
train.index <- createDataPartition(bank.df$Personal.Loan, 
                                   p = 0.8,
                                   list = F)  
train.df <- bank.df[train.index, ]
valid.df <- bank.df[-train.index, ]

```

We will do a 10 fold cross-validation to obtain our model. We will first define the `trainControl()` function for this resampling procedure.

```{r}
logit.reg <- glm(Personal.Loan ~ ., data = train.df, family = "binomial") 

summary(logit.reg)

tidy(logit.reg)
```

The estimates give us the impact of a unit change in the x variable on *log odds* of accepting a personal loan. For interpretation, we obtain the odds ratios.

```{r}
tidy(logit.reg, exponentiate = T)

result.1 <- tidy(logit.reg, exponentiate = T)

result.1 %>% 
  mutate(estimate = round(estimate,2),
         p.value = round(p.value, 4))
```

A number greater than 1 indicates that the factor increases the odds of the sample being in class 1.

Predictions: Remember the predictions are a probability. Give the option `type = "response"` to obtain the probabilities.

```{r}
trainpred.df <- augment(logit.reg,
                data = train.df,
                type.predict = "response")



logit.reg.trpred <- predict(logit.reg, type = "response")
logit.reg.trpred
```
