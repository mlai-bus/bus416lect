---
title: "ch05-01-preprocess"
format: html
editor: visual
---

## 5.1: Performance Evaluation of continuous outcomes

### Data partitioning

We saw that the base R function `sample()` does a good job. But my preference is a function built into the `caret` package.

Let us use the Boston dataset, to illustrate partitioning. The function `createDataPartition()` will create simple partitions of the dataset

```{r}
#| echo = false
library(tidyverse)
library(caret)
library(forecast)
library(readr)
library(broom)
library(mlbadata)
```

Load the data and create a 60-40 partition. We will set the random seed to a number to replicate the partition

```{r}
boston.df <- mlbadata::BostonHousing
boston.df <- boston.df |> 
  rename(High.Medv = CAT.MEDV)

set.seed(12345)
trainInd <- createDataPartition(c(1:nrow(boston.df)),
                                p = 0.6,
                                list = F)
boston.tr <- boston.df[trainInd, ]
boston.valid <- boston.df[-trainInd, ]
nrow(boston.tr)
nrow(boston.valid)
```

### Evaluating the Performance of Regression Models

There are three main prediction accuracy measures:

Mean Error (ME) $= \frac{1}{n} \sum^n_{i=1}e_{i}$

**Root Mean Square Error (RMSE)** $$ = \sqrt{\frac{1}{n}\sum^n_{i=1}e_{i}^2}$$

Mean Absolute Error (MAE) $= \frac{1}{n} \sum^n_{i=1}|e_{i}|$

Mean Percentage Error $$ = 100 \times \frac{1}{n} \sum^{n}_{i=1}\frac{e_{i}}{y_{i}} $$

Mean Absolute Percentage Error $$ = 100 \times \frac{1}{n} \sum^{n}_{i=1}|\frac{e_{i}}{y_{i}}| $$

We can obtain these from the `forecast()` package. Let us run a regression model on the Boston dataset predicting MEDV as a function of CRIM, CHAS and RM

```{r}
model.1 <- lm(MEDV ~ . , data = boston.tr[, c(1, 4, 6, 13)],
               na.action = na.omit)
#Get regression results
tidy(model.1)
#Get goodness of fit measures
glance(model.1)


```

Use the above model to predict the `MEDV` of the training and validation sets:

```{r}
medv_tr <- predict(model.1, na.action = na.omit)

medv_val <- predict(model.1, 
                    newdata = boston.valid,
                    na.action = na.omit)
```

Note that in the above code, we didn't have to specify the variables to be used. Now. let us compare the various predictive accuracy measures for both, the training and validation set MEDV values. We will use the `accuracy()` function from the `forecast` package. The first argument is the **predicted value** and the second argument is the **actual value**

```{r}
perf_train <- data.frame(accuracy(medv_tr, boston.tr$MEDV))
perf_valid <- data.frame(accuracy(medv_val, boston.valid$MEDV))
rbind(tr = perf_train , val = perf_valid )
```

In general, when we are running a variety of models, collecting all the accuracy measures in one dataset will be helpful in case we want to sort the models based on these stats.
