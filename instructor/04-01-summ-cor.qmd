---
title: "04-01-summ-cor"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Cleaning and Preliminary Diagnostics

### Learning objectives

After completing this section, you should be able to:

* Inspect and interpret descriptive statistics to understand data structure
* Convert categorical variables into factors suitable for modeling
* Create dummy variables and interaction terms using `caret`
* Diagnose multicollinearity using correlation analysis
* Reduce dimensionality by removing highly correlated predictors

These steps form the **foundation for any regression or machine learning model**.

---

## Load required libraries

```{r}
#| echo: false
library(tidyverse)
library(caret)
library(corrplot)
library(psych)
library(hablar)
library(skimr)
library(purrr)
library(readr)
library(patchwork)
library(mlbadata)
```

---

## Load and prepare the dataset

```{r}
housing.df <- mlbadata::BostonHousing
```

Two variables in this dataset are binary indicators:

* `CHAS` (Charles River dummy)
* `CAT.MEDV` (categorized median value)

For modeling purposes, these should be treated as **factors**, not numeric variables.

```{r}
cols <- c("CHAS", "CAT.MEDV")

housing.df[cols] <- map(
  housing.df[cols],
  factor,
  levels = c(1, 0),
  labels = c("Yes", "No")
)

head(housing.df)
```

### Why this matters

* Factors preserve categorical meaning
* Many modeling tools (including `caret`) rely on factor levels
* The **first level** is treated as the reference class

This choice will matter later when interpreting coefficients and predictions.

---

### Student exercise: factor conversion

```{r}
#| eval: false
# Use glimpse() to confirm:
# 1. Which variables are factors
# 2. Which variables remain numeric
```

---

## Initial data inspection

Before modeling, we examine basic descriptive statistics.

```{r}
summary(housing.df)
```

A more readable alternative is:

```{r}
skim(housing.df)
```

We can also customize what statistics are displayed.

```{r}
my_skim <- skim_with(numeric = sfl(p25 = NULL, p75 = NULL))
my_skim(housing.df)
```

### Interpretation guidance

At this stage, you should be asking:

* Are there extreme values?
* Are scales comparable?
* Do some variables appear highly skewed?

---

## Simple summaries and pivot-style tables

```{r}
housing.df |> 
  summarise(avgVal = mean(MEDV, na.rm = TRUE))
```

Grouped summaries resemble Excel pivot tables:

```{r}
housing.df |> 
  group_by(RAD, CHAS) |> 
  summarise(
    averageValue = mean(MEDV, na.rm = TRUE),
    count = n()
  )
```

These summaries help us understand **group-level patterns** before modeling.

---

### Student exercise: grouped summaries

```{r}
#| eval: false
# Compute the average MEDV grouped by:
# 1. CAT.MEDV
# 2. CHAS
```

---

## Creating dummy variables and interactions

Many ML algorithms require **numeric predictors**.

We begin by creating dummy variables for `CHAS` and an interaction with `RM`.

```{r}
df1 <- dummyVars(~ CHAS + CHAS:RM, data = housing.df)
dummies.df1 <- predict(df1, newdata = housing.df)
```

```{r}
head(dummies.df1)
```

Notice that:

* one dummy is created for **each category**
* this introduces perfect multicollinearity if all are included

This is known as the **dummy variable trap**.

---

## Enforcing full rank

To avoid multicollinearity, we request a **full-rank design matrix**.

```{r}
df2 <- dummyVars(
  ~ CHAS + CHAS:RM,
  data = housing.df,
  fullRank = TRUE
)

dummies.df2 <- predict(df2, newdata = housing.df)
head(dummies.df2)
```

Now, only one category per factor is retained.

---

### Student exercise: dummy variables

```{r}
#| eval: false
# How many columns were created:
# 1. Before fullRank = TRUE?
# 2. After fullRank = TRUE?
```

---

## Combining dummy variables with original data

```{r}
housing.df1 <- cbind(housing.df, dummies.df2) |> 
  select(-CHAS)

head(housing.df1)
```

This dataset is now suitable for regression-based models.

---

## Dimension reduction via correlation analysis

Highly correlated predictors can cause:

* unstable coefficient estimates
* inflated standard errors
* poor interpretability

We begin by separating:

* dependent variables
* candidate predictors

```{r}
yvars <- housing.df |> 
  select(MEDV, CAT.MEDV)

xvars <- housing.df |> 
  select(-c(MEDV, CAT.MEDV))
```

Only numeric predictors are used for correlation analysis.

```{r}
xvarsnum <- xvars |> 
  select_if(is.numeric)
```

---

## Correlation matrix and visualization

```{r}
cor1 <- cor(xvarsnum)
corrplot(cor1)
```

```{r}
corrplot(cor1, method = "number")
corrplot(cor1, order = "hclust", method = "number")
```

These plots help identify **clusters of highly correlated variables**.

```{r}
ggpairs(xvarsnum[, c(1, 3, 4, 6)])
```

---

### Student exercise: correlation diagnostics

```{r}
#| eval: false
# Identify one pair of variables
# with visibly high correlation
```

---

## Automated correlation filtering

`caret` provides a utility function for removing highly correlated predictors.

```{r}
high.corr <- findCorrelation(cor1, cutoff = 0.8)
high.corr
```

```{r}
t(names(xvarsnum[high.corr]))
```

The variable identified here is removed.

```{r}
xvarfin <- xvarsnum[, -high.corr]
```

---

## Final modeling dataset

```{r}
df1 <- cbind(
  MEDV = yvars$MEDV,
  CHAS = xvars$CHAS,
  xvarfin
)

head(df1)
```

This dataset:

* avoids multicollinearity
* preserves key categorical structure
* is ready for model training

---

## Key takeaways

* Data cleaning is **model preparation**
* Factor encoding decisions affect interpretation
* Multicollinearity must be addressed *before* regression type modeling
* Correlation analysis supports principled feature reduction


